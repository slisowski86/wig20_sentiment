{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08d39aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adjustText in /home/slisowski/.local/lib/python3.9/site-packages (0.7.3)\n",
      "Requirement already satisfied: numpy in /home/slisowski/.local/lib/python3.9/site-packages (from adjustText) (1.22.2)\n",
      "Requirement already satisfied: matplotlib in /home/slisowski/.local/lib/python3.9/site-packages (from adjustText) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->adjustText) (8.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 16:53:29.580460: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-27 16:53:29.580486: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "!pip install adjustText\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from operator import itemgetter\n",
    "all_files=listdir('/home/slisowski/Portfolio/wig20_sentiment/translated_files_prod/embed/')\n",
    "\n",
    "#We have to sort files by index number, but this is string so first we need to extract index as integer\n",
    "file_idx=[]\n",
    "for file in all_files:\n",
    "    idx=file.split('_')[2]\n",
    "    file_idx.append(int(idx))\n",
    "file_idx\n",
    "\n",
    "files_idx=zip(all_files, file_idx)\n",
    "sorted_tuples=sorted(tuple(files_idx),key=itemgetter(1))\n",
    "sorted_list_files=[]\n",
    "for tup in sorted_tuples:\n",
    "    dir_path='/home/slisowski/Portfolio/wig20_sentiment/translated_files_prod/embed/'\n",
    "    file_path=dir_path+tup[0]\n",
    "    sorted_list_files.append(file_path)\n",
    "    \n",
    "\n",
    "    \n",
    "news_df=pd.concat(map(pd.read_csv, sorted_list_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18dd79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df=pd.read_csv('/home/slisowski/Portfolio/tweets_news_sentiment/tweets_labelled.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489f51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headlines_df=pd.read_csv('/home/slisowski/Portfolio/tweets_news_sentiment/all-data.csv', encoding=\"Windows-1252\", names=[\"sentiment\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cdb2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_corpus=pd.concat([news_df['news_text'], tweets_df['text'], news_headlines_df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e2f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import preprocessor as p\n",
    "from bs4 import BeautifulSoup\n",
    "import demoji\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clean_news_cbow(text):\n",
    "    \n",
    "    #remove emoji\n",
    "    text=demoji.replace(text,' ')\n",
    "    # use tweeter preprocessor to clean news\n",
    "    text=p.clean(text)\n",
    "    #remove html tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text=contractions.fix(text)\n",
    "    \n",
    "    \n",
    "    #remove accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "   \n",
    "   \n",
    "    #remove numbers\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    #tokenize text\n",
    "    text=text.lower()\n",
    "    \n",
    "    list_of_words=word_tokenize(text)\n",
    "    #words=[]\n",
    "    \n",
    "    text = ' '.join(list_of_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "649f1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_words=[]\n",
    "for news in cbow_corpus.tolist():\n",
    "    \n",
    "    text=clean_news_cbow(news)\n",
    "    cbow_words.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186c434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_words=list(set(cbow_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0c1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stories=cbow_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db22061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a5b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)\n",
    "news_sequences=[x for x in news_sequences if len(x)>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d9f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
    "\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa5489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_grams(sequence, vocabulary_size,\n",
    "              window_size=4, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "    \n",
    "    targets, contexts, labels = [], [], []    \n",
    "        \n",
    "    for i, wi in enumerate(sequence):\n",
    "        \n",
    "        \n",
    "        if not wi or i < window_size or i + 1 > len(sequence)-window_size:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        \n",
    "        context_words = [wj for j, wj in enumerate(sequence[window_start:window_end]) if j+window_start != i]\n",
    "        target_word = wi        \n",
    "            \n",
    "        context_classes = tf.expand_dims(tf.constant(context_words, dtype=\"int64\"), 0)\n",
    "\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_classes,\n",
    "          num_true=window_size * 2,\n",
    "          num_sampled=negative_samples,\n",
    "          unique=True,\n",
    "          range_max=vocabulary_size,              \n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)        \n",
    "        negative_targets = negative_sampling_candidates.numpy().tolist()        \n",
    "\n",
    "        target = [target_word] + negative_targets\n",
    "        label = [1] + [0]*negative_samples\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.extend(target)\n",
    "        contexts.extend([context_words]*(negative_samples+1))\n",
    "        labels.extend(label)\n",
    "        \n",
    "    couples = list(zip(targets, contexts))\n",
    "    \n",
    "    seed = random.randint(0, 10e6)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(couples)    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(labels)\n",
    "           \n",
    "    \n",
    "    return couples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d6e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "epochs = 10 # Number of epochs to train for\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ba91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cbow_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 2, 128)       2660224     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          2660224     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 2)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,320,448\n",
      "Trainable params: 5,320,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 16:57:09.408876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-27 16:57:09.408910: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-27 16:57:09.408935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (slawomir): /proc/driver/nvidia/version does not exist\n",
      "2022-09-27 16:57:09.409107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# Inputs; target input layer will have the final shape [None]\n",
    "# context will have [None, 2xwindow_size] shape\n",
    "input_1 = tf.keras.layers.Input(shape=())\n",
    "input_2 = tf.keras.layers.Input(shape=(window_size*2,))\n",
    "\n",
    "# Target and context embedding layers\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "\n",
    "# Outputs of the target and context embedding lookups\n",
    "context_out = context_embedding_layer(input_2)\n",
    "target_out = target_embedding_layer(input_1)\n",
    "\n",
    "# Taking the mean over the all the context words to produce [None, embedding_size]\n",
    "mean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out)\n",
    "\n",
    "# Computing the dot product between the two \n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model')\n",
    "\n",
    "cbow_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93781375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_data_generator(sequences, window_size, batch_size, negative_samples):\n",
    "    \n",
    "    rand_sequence_ids = np.arange(len(sequences))                    \n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "        inputs, labels = cbow_grams(\n",
    "            sequences[si], \n",
    "            vocabulary_size=n_vocab, \n",
    "            window_size=window_size, \n",
    "            negative_samples=negative_samples, \n",
    "            shuffle=True,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=None\n",
    "        )\n",
    "        \n",
    "        inputs_context, inputs_target, labels = np.array([inp[1] for inp in inputs]), np.array([inp[0] for inp in inputs]), np.array(labels).reshape(-1,1)\n",
    "        \n",
    "        assert inputs_context.shape[0] == inputs_target.shape[0]\n",
    "        assert inputs_context.shape[0] == labels.shape[0]\n",
    "        \n",
    "        #print(inputs_context.shape, inputs_target.shape, labels.shape)\n",
    "        for eg_id_start in range(0, inputs_context.shape[0], batch_size):            \n",
    "            \n",
    "            yield (\n",
    "                inputs_target[eg_id_start: min(eg_id_start+batch_size, inputs_target.shape[0])], \n",
    "                inputs_context[eg_id_start: min(eg_id_start+batch_size, inputs_context.shape[0]),:]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f066e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "        \n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "                \n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "        \n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "        \n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "                \n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "            \n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5c9bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 started\n",
      "  16614/Unknown - 985s 59ms/step - loss: 0.4758amount: echo, trademark, proprietary, sharp, hod\n",
      "term: innovation, without, run, handlowy, cds\n",
      "s: moves, maker, assumptions, pivots, gmv\n",
      "stock: court, old, day, s, janw\n",
      "first: second, third, fourth, inch, of\n",
      "compared: collection, fo, oriola, finishing, trump\n",
      "most: zm, zygmunt, bynd, transportation, primarily\n",
      "investment: airlines, program, subordinate, lot, scheduled\n",
      "pkn: wants, daniel, vitay, finished, secured\n",
      "loss: profit, consolidated, seen, operations, income\n",
      "pekao: surprising, elektrociepownia, hinting, jaston, conquered\n",
      "markets: rmf, mins, prague, largely, dimon\n",
      "sector: the, polish, statistical, level, gwent\n",
      "pge: grupa, vcit, live, chose, stated\n",
      "data: kubica, expectations, agricultural, comprise, appeared\n",
      "profit: loss, result, consolidated, sales, errors\n",
      "former: iii, structure, independent, branch, sells\n",
      "mr: foundries, joining, stake, biib, exel\n",
      "transactions: appeared, oct, kazakhstan, caution, silicon\n",
      "receive: dgly, workers, political, charger, test\n",
      "expansion: marshall, audjpy, psx, twice, miek\n",
      "norway: certain, flexible, scheme, uptrend, canada\n",
      "proceedings: nen, alerted, miek, yr, qsr\n",
      "prepared: cheap, affect, mnta, hell, wasiek\n",
      "lead: fallen, matching, aytu, duty, crypto\n",
      "poor: prague, man, miek, specialty, valuation\n",
      "cut: led, dependent, inter, smoking, emeryk\n",
      "stay: tlt, ordinary, vessels, saturday, found\n",
      "show: stake, fallen, techniques, suspect, audjpy\n",
      "force: might, numbers, tusk, environmentally, gnc\n",
      "maintenance: fallen, marshall, sawomir, certain, pak\n",
      "neutral: approach, lmt, substantial, certain, tauron\n",
      "\n",
      "\n",
      "16614/16614 [==============================] - 985s 59ms/step - loss: 0.4758\n",
      "Epoch: 2/10 started\n",
      "  16667/Unknown - 956s 57ms/step - loss: 0.4016amount: a, mounting, stable, advantage, tone\n",
      "term: sized, horizon, lasting, cds, electricity\n",
      "s: private, elisa, offered, maker, decisions\n",
      "stock: ends, court, rate, indexes, gpw\n",
      "first: second, fourth, third, of, inch\n",
      "compared: eur, usd, dominant, sek, according\n",
      "most: appear, signal, overcoming, returns, zm\n",
      "investment: opening, commission, sheet, economist, equities\n",
      "pkn: daniel, patrycja, recommends, wants, plans\n",
      "loss: profit, taxes, income, consolidated, sales\n",
      "pekao: pkobp, polski, indicate, simple, lincoln\n",
      "markets: conquer, project, others, clicks, longs\n",
      "sector: direction, site, destroy, center, electronic\n",
      "pge: grupa, mied, imputed, returning, kghm\n",
      "data: maximum, decisions, formula, meeting, fluctuated\n",
      "profit: loss, consolidated, result, sales, income\n",
      "former: complex, flexi, shareholding, dollar, inflationary\n",
      "mr: infr, kru, flush, rao, thin\n",
      "transactions: sappi, janno, redesigned, akebia, consolidate\n",
      "receive: parliamentary, representation, bpth, totals, demonstration\n",
      "expansion: thene, crac, performers, swedbank, standards\n",
      "norway: kr, crwd, certain, lee, iova\n",
      "proceedings: imposing, comp, iphones, estimate, moderately\n",
      "prepared: fri, atmosphere, bashers, amcor, southfield\n",
      "lead: along, aytu, qcom, face, certain\n",
      "poor: ta, mickey, clock, shelters, beating\n",
      "cut: deadline, atthemoney, halving, travels, utilises\n",
      "stay: tap, bluerock, bmo, benches, mom\n",
      "show: khan, xlm, cuautitlan, consolidating, msm\n",
      "force: formal, frustration, apo, abc, discipline\n",
      "maintenance: rf, stanisawa, audjpy, coping, ia\n",
      "neutral: pandemic, safety, coking, pcs, fhbc\n",
      "\n",
      "\n",
      "16667/16667 [==============================] - 957s 57ms/step - loss: 0.4016\n",
      "Epoch: 3/10 started\n",
      "  16623/Unknown - 1025s 62ms/step - loss: 0.3649amount: nominal, number, be, stable, a\n",
      "term: esm, sized, instruction, awaited, cds\n",
      "s: offered, elisa, coq, confirmed, maker\n",
      "stock: ends, gpw, rate, dataswarm, announcement\n",
      "first: second, fourth, third, contactless, gemius\n",
      "compared: eur, dominant, ragot, usd, bearing\n",
      "most: step, dickens, appear, hint, webcast\n",
      "investment: portfolios, dimensional, di, own, propylene\n",
      "pkn: daniel, patrycja, submitted, emphasizes, wocawski\n",
      "loss: profit, income, result, taxes, consolidated\n",
      "pekao: pkobp, mbank, pko, pzu, initially\n",
      "markets: scenario, conquer, exchanges, stride, others\n",
      "sector: leader, typobage, agri, omxn, deposits\n",
      "pge: grupa, ccc, pknnorlen, lpp, kghm\n",
      "data: readings, reading, vimpressive, thick, agricultural\n",
      "profit: loss, result, income, consolidated, taxes\n",
      "former: independent, inflationary, shareholding, pandemic, known\n",
      "mr: dvn, sears, cron, kelly, reels\n",
      "transactions: letter, caution, inspections, finale, virtue\n",
      "receive: axas, netbooks, myth, nigeria, mlco\n",
      "expansion: hits, tapes, standards, fined, aitrade\n",
      "norway: cynically, gazownictwo, llc, stations, nominally\n",
      "proceedings: ggal, hoc, khc, bubble, zhud\n",
      "prepared: less, timelines, netflix, compiling, hope\n",
      "lead: reflecting, usdt, ixic, animal, chartered\n",
      "poor: ta, breakdown, excellent, neutral, good\n",
      "cut: limits, watertight, bhushan, centralized, curves\n",
      "stay: srna, wedbush, crane, highly, poorly\n",
      "show: wba, face, dlpn, parent, bsv\n",
      "force: fcf, recession, redemptions, packed, fraudsters\n",
      "maintenance: broadband, leading, technology, tools, machinery\n",
      "neutral: average, breakdown, escalation, phil, jumps\n",
      "\n",
      "\n",
      "16623/16623 [==============================] - 1025s 62ms/step - loss: 0.3649\n",
      "Epoch: 4/10 started\n",
      "  16634/Unknown - 1014s 61ms/step - loss: 0.3378amount: be, number, nominal, achieve, pear\n",
      "term: esm, sized, sabbatical, awaited, references\n",
      "s: elisa, armed, offered, consolidated, corporation\n",
      "stock: ends, dataswarm, dance, old, processors\n",
      "first: second, fourth, third, sided, gradual\n",
      "compared: eur, usd, dominant, ragot, honka\n",
      "most: srk, element, step, midday, hint\n",
      "investment: dkr, portfolios, midas, di, own\n",
      "pkn: daniel, patrycja, shortened, wants, armen\n",
      "loss: profit, taxes, income, result, meur\n",
      "pekao: mbank, pko, pkobp, indicate, assess\n",
      "markets: others, conquer, presence, initiatives, indexes\n",
      "sector: agri, leader, segment, certified, polish\n",
      "pge: grupa, mied, pknnorlen, kghm, lpp\n",
      "data: reading, vimpressive, factors, screening, readings\n",
      "profit: loss, result, consolidated, taxes, income\n",
      "former: independent, kkr, danske, shareholding, hcl\n",
      "mr: spaq, kelly, patented, learned, hela\n",
      "transactions: multienerge, finally, caution, semi, consent\n",
      "receive: reka, rachesky, klaipedos, axas, columbus\n",
      "expansion: locomotive, modernization, clientele, bop, controversial\n",
      "norway: shedlin, iqiyi, keepin, evp, casualty\n",
      "proceedings: khc, hoc, pbsz, dziwulski, myos\n",
      "prepared: netflix, neon, compiling, makeover, floated\n",
      "lead: shalkiya, roaster, reflecting, kim, kardzhali\n",
      "poor: ta, breakdown, good, risky, excellent\n",
      "cut: limits, recall, chinas, pg, aerfugl\n",
      "stay: audiences, photogrammetric, jamali, srna, abg\n",
      "show: wba, khan, dlpn, bosc, mellody\n",
      "force: strength, benefon, converdcent, redemptions, mpcl\n",
      "maintenance: machinery, combust, communications, connectivity, leading\n",
      "neutral: average, breakdown, escalation, strike, ibris\n",
      "\n",
      "\n",
      "16634/16634 [==============================] - 1014s 61ms/step - loss: 0.3378\n",
      "Epoch: 5/10 started\n",
      "  16665/Unknown - 958s 58ms/step - loss: 0.3157amount: number, nominal, be, detection, reaching\n",
      "term: sized, esm, sabbatical, raids, references\n",
      "s: armed, elisa, jerome, vmc, maggie\n",
      "stock: ends, dance, fintech, infoengine, court\n",
      "first: second, fourth, third, vocational, careers\n",
      "compared: nearly, prizes, honka, pct, ragot\n",
      "most: step, equally, overcoming, exercize, element\n",
      "investment: dkr, di, stabilization, capex, bahia\n",
      "pkn: daniel, wants, submitted, patrycja, fulfilled\n",
      "loss: profit, result, taxes, income, meur\n",
      "pekao: mbank, pkobp, polski, pko, pzu\n",
      "markets: exchanges, pivots, points, scenario, errors\n",
      "sector: agri, leader, manufacturers, segment, owner\n",
      "pge: grupa, mied, pknnorlen, lpp, pgnig\n",
      "data: reading, screening, vimpressive, prospects, dimensional\n",
      "profit: loss, result, consolidated, income, taxes\n",
      "former: doering, independent, stoen, substance, aims\n",
      "mr: tuomainen, viitaniemi, helander, katzman, saavalainen\n",
      "transactions: finally, multienerge, mazowieckie, caution, situation\n",
      "receive: generate, paying, madden, predicts, for\n",
      "expansion: modernization, maritime, improvement, locomotive, alternative\n",
      "norway: pgnig, shedlin, buenaventura, gazownictwo, bochum\n",
      "proceedings: finnfund, perttu, dinh, bajaj, compressors\n",
      "prepared: thornton, compiling, neon, convinced, turbo\n",
      "lead: iep, rht, cmg, kr, roaster\n",
      "poor: ta, good, risky, breakdown, conservative\n",
      "cut: limits, buy, price, close, highlight\n",
      "stay: celtf, bysi, spolka, flip, semiconductors\n",
      "show: webcast, feet, close, incur, ask\n",
      "force: strength, damaged, applies, redemptions, mpcl\n",
      "maintenance: connectivity, combust, machinery, dosing, transportatn\n",
      "neutral: breakdown, fa, average, categories, irtc\n",
      "\n",
      "\n",
      "16665/16665 [==============================] - 958s 58ms/step - loss: 0.3157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 started\n",
      "  16621/Unknown - 1015s 61ms/step - loss: 0.2974amount: number, nominal, saturday, burner, edges\n",
      "term: sized, raids, esm, sabbatical, lasting\n",
      "s: armed, megafon, elisa, maggie, iphi\n",
      "stock: dataswarm, infoengine, fintech, eols, dance\n",
      "first: second, fourth, third, framework, biscuit\n",
      "compared: eur, nearly, sek, pct, honka\n",
      "most: step, equally, epicenter, iko, exercize\n",
      "investment: dkr, pfr, di, portfolios, society\n",
      "pkn: daniel, wants, pgniig, submitted, patrycja\n",
      "loss: profit, result, taxes, meur, income\n",
      "pekao: pkobp, mbank, polski, pko, pzu\n",
      "markets: indexes, scenario, curve, exchanges, errors\n",
      "sector: agri, leader, electronic, featured, conventional\n",
      "pge: grupa, mied, pknnorlen, lpp, pgnig\n",
      "data: published, communication, reading, screener, environment\n",
      "profit: loss, consolidated, result, taxes, income\n",
      "former: pth, doering, authorized, stoen, caixabank\n",
      "mr: tuomainen, katzman, helander, viitaniemi, niklas\n",
      "transactions: multienerge, mazowieckie, caution, negotiate, switch\n",
      "receive: pay, tender, paying, predicts, lenovo\n",
      "expansion: improvement, reliable, transferred, alternative, intelligent\n",
      "norway: pgnig, gazownictwo, shedlin, bochum, atria\n",
      "proceedings: perttu, tribunal, finnfund, petri, tyrv\n",
      "prepared: nato, compiling, makeover, outlisten, mexican\n",
      "lead: powerhouse, rht, pe, roaster, capacity\n",
      "poor: conservative, good, breakdown, ta, risky\n",
      "cut: recall, paijat, gallerix, highlight, limits\n",
      "stay: celtf, bysi, similarly, brisker, spolka\n",
      "show: worse, overbought, dlpn, incur, similarly\n",
      "force: validity, damaged, strapping, bzb, appearances\n",
      "maintenance: procurement, documentation, machinery, connectivity, combust\n",
      "neutral: breakdown, fa, categories, ta, mdc\n",
      "\n",
      "\n",
      "16621/16621 [==============================] - 1015s 61ms/step - loss: 0.2974\n",
      "Epoch: 7/10 started\n",
      "  16597/Unknown - 949s 57ms/step - loss: 0.2807amount: number, be, saturday, achieve, burner\n",
      "term: esm, sized, sabbatical, balance, raids\n",
      "s: megafon, refinance, maggie, nobel, makuisa\n",
      "stock: dataswarm, eols, infoengine, seaspine, fintech\n",
      "first: second, third, fourth, framework, biscuit\n",
      "compared: sek, nearly, eur, pledge, corresponding\n",
      "most: equally, plaque, ribbons, step, exercize\n",
      "investment: dkr, portfolios, vid, biocidal, di\n",
      "pkn: daniel, pgniig, patrycja, bartos, wants\n",
      "loss: profit, result, taxes, meur, consolidated\n",
      "pekao: pkobp, mbank, santander, pko, polski\n",
      "markets: scenario, exchanges, indexes, omi, dipping\n",
      "sector: agri, flats, typobage, mit, mop\n",
      "pge: grupa, mied, lpp, pknnorlen, pgnig\n",
      "data: eyen, results, published, communication, reading\n",
      "profit: loss, consolidated, result, taxes, meur\n",
      "former: pth, authorized, stoen, doering, caixabank\n",
      "mr: tuomainen, helander, katzman, viitaniemi, niklas\n",
      "transactions: slowdown, mazowieckie, situation, multienerge, caution\n",
      "receive: pay, lenovo, paying, involved, excludes\n",
      "expansion: alternative, intelligent, reliable, licensing, tvs\n",
      "norway: gazownictwo, pgnig, bochum, shedlin, estimating\n",
      "proceedings: perttu, frankly, illustrators, pte, petri\n",
      "prepared: nato, netflix, gets, criminals, thornton\n",
      "lead: roaster, powerhouse, shalkiya, norge, bigs\n",
      "poor: conservative, good, ta, breakdown, grateful\n",
      "cut: outside, mostly, paijat, gallerix, defend\n",
      "stay: celtf, bysi, neuki, similarly, latam\n",
      "show: incur, overbought, careful, break, become\n",
      "force: validity, bzb, strapping, strength, appearances\n",
      "maintenance: procurement, machinery, connectivity, forest, innovative\n",
      "neutral: breakdown, technical, day, average, fa\n",
      "\n",
      "\n",
      "16597/16597 [==============================] - 949s 57ms/step - loss: 0.2807\n",
      "Epoch: 8/10 started\n",
      "  16604/Unknown - 943s 57ms/step - loss: 0.2672amount: number, be, achieve, saturday, nominal\n",
      "term: esm, regulates, raids, sabbatical, ciders\n",
      "s: maggie, makuisa, eberl, lechosaw, armed\n",
      "stock: dataswarm, seaspine, infoengine, eols, currency\n",
      "first: second, third, fourth, biscuit, last\n",
      "compared: eur, sek, zlotys, nearly, ragot\n",
      "most: element, equally, epicenter, step, exercize\n",
      "investment: dkr, portfolios, stabilization, vid, care\n",
      "pkn: daniel, pgniig, fees, model, krupiski\n",
      "loss: profit, taxes, result, meur, income\n",
      "pekao: pkobp, polski, mbank, santander, pzu\n",
      "markets: exchanges, indexes, valga, scenario, points\n",
      "sector: agri, potato, industry, typobage, format\n",
      "pge: grupa, mied, pknnorlen, lpp, pgnig\n",
      "data: communication, reading, eyen, published, results\n",
      "profit: loss, consolidated, result, taxes, income\n",
      "former: pth, authorized, mikkonen, caixabank, monika\n",
      "mr: katzman, tuomainen, helander, zoltan, saavalainen\n",
      "transactions: mazowieckie, multienerge, slowdown, occupancy, situation\n",
      "receive: pay, lenovo, involved, cents, eurchf\n",
      "expansion: alternative, improvement, escalation, heating, mill\n",
      "norway: gazownictwo, pgnig, withviking, shedlin, bochum\n",
      "proceedings: frankly, illustrators, perttu, petri, motors\n",
      "prepared: nato, saying, gets, no, diversity\n",
      "lead: roaster, powerhouse, pe, kardzhali, shalkiya\n",
      "poor: conservative, good, chasm, ta, breakdown\n",
      "cut: region, gallerix, substantial, marijuana, paijat\n",
      "stay: celtf, lotsnot, bysi, audiences, brisker\n",
      "show: careful, overbought, present, honestly, break\n",
      "force: validity, strength, bzb, collins, excluded\n",
      "maintenance: procurement, sourcing, machinery, electronics, tb\n",
      "neutral: breakdown, fa, ta, setup, ibris\n",
      "\n",
      "\n",
      "16605/16605 [==============================] - 943s 57ms/step - loss: 0.2672\n",
      "Epoch: 9/10 started\n",
      "  16655/Unknown - 944s 57ms/step - loss: 0.2581amount: achieve, number, jfin, be, burner\n",
      "term: esm, raids, regulates, sized, headline\n",
      "s: lechosaw, makuisa, maggie, eberl, iphi\n",
      "stock: dataswarm, seaspine, eols, currency, coin\n",
      "first: second, third, fourth, biscuit, last\n",
      "compared: eur, sek, pct, pledge, ragot\n",
      "most: equally, element, exercize, plaque, persist\n",
      "investment: dkr, portfolios, vitelo, evli, stabilization\n",
      "pkn: daniel, model, encompassing, patrycja, stands\n",
      "loss: profit, meur, taxes, result, consolidated\n",
      "pekao: polski, pkobp, mbank, santander, skiba\n",
      "markets: exchanges, indexes, eriksson, inves, valga\n",
      "sector: agri, opinia, helplines, flats, typobage\n",
      "pge: grupa, pknnorlen, mied, lpp, kghm\n",
      "data: communication, reading, published, results, eyen\n",
      "profit: loss, consolidated, result, taxes, purified\n",
      "former: pth, authorized, foremost, monika, mikkonen\n",
      "mr: katzman, helander, zoltan, saavalainen, tuomainen\n",
      "transactions: multienerge, situation, mazowieckie, wschorowska, slowdown\n",
      "receive: pay, involved, predicts, lenovo, cents\n",
      "expansion: memorandum, reliable, escalation, struggle, tendency\n",
      "norway: withviking, egyptian, portugal, shedlin, gazownictwo\n",
      "proceedings: frankly, kety, perttu, tribunal, petri\n",
      "prepared: nato, diversity, benzina, thing, kocian\n",
      "lead: powerhouse, roaster, plains, shalkiya, hikes\n",
      "poor: chasm, conservative, good, breakdown, ta\n",
      "cut: paijat, gallerix, region, average, stockpile\n",
      "stay: celtf, lotsnot, brisker, catalyst, adoption\n",
      "show: careful, confident, overbought, nonlogarithmic, break\n",
      "force: validity, excluded, bzb, strength, appearances\n",
      "maintenance: sourcing, electronics, procurement, connections, tb\n",
      "neutral: breakdown, ta, fa, chainlink, stride\n",
      "\n",
      "\n",
      "16655/16655 [==============================] - 944s 57ms/step - loss: 0.2581\n",
      "Epoch: 10/10 started\n",
      "  16567/Unknown - 965s 58ms/step - loss: 0.2466amount: achieve, number, be, burner, donation\n",
      "term: esm, regulates, raids, maxima, sized\n",
      "s: eberl, maggie, motilal, makuisa, floral\n",
      "stock: dataswarm, seaspine, bulletin, eols, currency\n",
      "first: second, fourth, third, coronavir, biscuit\n",
      "compared: eur, sek, pekabex, pledge, zlotys\n",
      "most: exercize, equally, downloaded, overcoming, mood\n",
      "investment: vitelo, dkr, portfolios, evli, stabilization\n",
      "pkn: daniel, patrycja, synthos, pgniig, emphasizes\n",
      "loss: profit, meur, taxes, result, consolidated\n",
      "pekao: polski, pkobp, santander, mbank, pko\n",
      "markets: exchanges, inves, eriksson, dipping, scenario\n",
      "sector: agri, opinia, helplines, mop, culprit\n",
      "pge: grupa, pknnorlen, mied, lpp, chmielewski\n",
      "data: communication, reading, screener, results, screening\n",
      "profit: loss, result, consolidated, taxes, purified\n",
      "former: pth, authorized, monika, mikkonen, foremost\n",
      "mr: katzman, helander, tuomainen, saavalainen, zoltan\n",
      "transactions: slowdown, pressure, multienerge, situation, dog\n",
      "receive: pay, excludes, marketplace, popularize, holes\n",
      "expansion: alternative, acquisitions, improvement, memorandum, location\n",
      "norway: pgnig, egyptian, gazownictwo, bayern, portugal\n",
      "proceedings: frankly, petri, significance, derives, compressors\n",
      "prepared: nato, elektrociepownia, launched, multifunctional, brokerage\n",
      "lead: powerhouse, roaster, chu, ministerial, hikes\n",
      "poor: conservative, breakdown, chasm, good, grateful\n",
      "cut: gallerix, yes, getting, paijat, average\n",
      "stay: lotsnot, celtf, minds, sick, greedy\n",
      "show: nonlogarithmic, overbought, confident, spilled, perversely\n",
      "force: validity, collins, excluded, stochastic, damaged\n",
      "maintenance: networking, lubricants, procurement, electronics, innovative\n",
      "neutral: fw, odyssey, breakdown, average, historic\n",
      "\n",
      "\n",
      "16567/16567 [==============================] - 965s 58ms/step - loss: 0.2466\n"
     ]
    }
   ],
   "source": [
    "cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    news_cbow_gen = cbow_data_generator(news_sequences, window_size, batch_size, negative_samples)\n",
    "    cbow_model.fit(\n",
    "        news_cbow_gen, \n",
    "        epochs=1, \n",
    "        callbacks=cbow_validation_callback,         \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04ee2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
    "        \n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"context_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"context_embedding_cbow.pkl\"))\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"target_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"target_embedding_cbiw.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d449b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(cbow_model, tokenizer, n_vocab, save_dir='cbow_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a46373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
