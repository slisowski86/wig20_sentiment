{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08d39aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adjustText in /home/slisowski/.local/lib/python3.9/site-packages (0.7.3)\r\n",
      "Requirement already satisfied: matplotlib in /home/slisowski/.local/lib/python3.9/site-packages (from adjustText) (3.5.2)\r\n",
      "Requirement already satisfied: numpy in /home/slisowski/.local/lib/python3.9/site-packages (from adjustText) (1.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (21.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (1.4.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (4.33.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->adjustText) (8.1.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (2.4.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/slisowski/.local/lib/python3.9/site-packages (from matplotlib->adjustText) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 08:09:45.133997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-20 08:09:45.134032: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "!pip install adjustText\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.read_csv(\"/home/slisowski/Portfolio/wig20_sentiment/news_prepared_to_model_th.csv\", index_col=0)\n",
    "news_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "news_stories=news_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db22061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None, split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a5b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)\n",
    "news_sequences=[x for x in news_sequences if len(x)>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d9f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
    "\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa5489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_grams(sequence, vocabulary_size,\n",
    "              window_size=4, negative_samples=1., shuffle=True,\n",
    "              categorical=False, sampling_table=None, seed=None):\n",
    "    \n",
    "    targets, contexts, labels = [], [], []    \n",
    "        \n",
    "    for i, wi in enumerate(sequence):\n",
    "        \n",
    "        \n",
    "        if not wi or i < window_size or i + 1 > len(sequence)-window_size:\n",
    "            continue\n",
    "        if sampling_table is not None:\n",
    "            if sampling_table[wi] < random.random():\n",
    "                continue\n",
    "\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        \n",
    "        context_words = [wj for j, wj in enumerate(sequence[window_start:window_end]) if j+window_start != i]\n",
    "        target_word = wi        \n",
    "            \n",
    "        context_classes = tf.expand_dims(tf.constant(context_words, dtype=\"int64\"), 0)\n",
    "\n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_classes,\n",
    "          num_true=window_size * 2,\n",
    "          num_sampled=negative_samples,\n",
    "          unique=True,\n",
    "          range_max=vocabulary_size,              \n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "        # Build context and label vectors (for one target word)        \n",
    "        negative_targets = negative_sampling_candidates.numpy().tolist()        \n",
    "\n",
    "        target = [target_word] + negative_targets\n",
    "        label = [1] + [0]*negative_samples\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.extend(target)\n",
    "        contexts.extend([context_words]*(negative_samples+1))\n",
    "        labels.extend(label)\n",
    "        \n",
    "    couples = list(zip(targets, contexts))\n",
    "    \n",
    "    seed = random.randint(0, 10e6)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(couples)    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(labels)\n",
    "           \n",
    "    \n",
    "    return couples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6e2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of 1 on either side of target word\n",
    "epochs = 5 # Number of epochs to train for\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ba91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cbow_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 2, 128)       1507968     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          1507968     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 2)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,015,936\n",
      "Trainable params: 3,015,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "# Inputs; target input layer will have the final shape [None]\n",
    "# context will have [None, 2xwindow_size] shape\n",
    "input_1 = tf.keras.layers.Input(shape=())\n",
    "input_2 = tf.keras.layers.Input(shape=(window_size*2,))\n",
    "\n",
    "# Target and context embedding layers\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "\n",
    "# Outputs of the target and context embedding lookups\n",
    "context_out = context_embedding_layer(input_2)\n",
    "target_out = target_embedding_layer(input_1)\n",
    "\n",
    "# Taking the mean over the all the context words to produce [None, embedding_size]\n",
    "mean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out)\n",
    "\n",
    "# Computing the dot product between the two \n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model')\n",
    "\n",
    "cbow_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93781375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_data_generator(sequences, window_size, batch_size, negative_samples):\n",
    "    \n",
    "    rand_sequence_ids = np.arange(len(sequences))                    \n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "        inputs, labels = cbow_grams(\n",
    "            sequences[si], \n",
    "            vocabulary_size=n_vocab, \n",
    "            window_size=window_size, \n",
    "            negative_samples=negative_samples, \n",
    "            shuffle=True,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=None\n",
    "        )\n",
    "        \n",
    "        inputs_context, inputs_target, labels = np.array([inp[1] for inp in inputs]), np.array([inp[0] for inp in inputs]), np.array(labels).reshape(-1,1)\n",
    "        \n",
    "        assert inputs_context.shape[0] == inputs_target.shape[0]\n",
    "        assert inputs_context.shape[0] == labels.shape[0]\n",
    "        \n",
    "        #print(inputs_context.shape, inputs_target.shape, labels.shape)\n",
    "        for eg_id_start in range(0, inputs_context.shape[0], batch_size):            \n",
    "            \n",
    "            yield (\n",
    "                inputs_target[eg_id_start: min(eg_id_start+batch_size, inputs_target.shape[0])], \n",
    "                inputs_context[eg_id_start: min(eg_id_start+batch_size, inputs_context.shape[0]),:]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f066e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "        \n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "                \n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "        \n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "        \n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "                \n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "            \n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c9bf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "   9901/Unknown - 293s 30ms/step - loss: 0.4961into: mark, papers, independent, revenue, finnish\n",
      "system: billing, region, ai, fishing, fight\n",
      "at: conduct, scheduled, offs, chart, made\n",
      "he: today, signing, music, adpnews, entire\n",
      "increase: euros, branch, imposed, buy, short\n",
      "customers: underway, fb, marine, reduction, euros\n",
      "data: fishing, save, lead, revenue, house\n",
      "pgnig: developed, dividends, machine, dow, underway\n",
      "have: has, sending, getting, kgn, subsequent\n",
      "related: each, my, car, contracts, please\n",
      "compared: breakout, light, try, coronavirus, shows\n",
      "among: directions, networks, kymmene, medium, g\n",
      "recommendation: technologies, container, euros, pan, dividends\n",
      "bank: location, themselves, offices, cars, my\n",
      "industry: cards, laboratory, inc, technology, employee\n",
      "this: dis, owned, which, starting, wall\n",
      "voting: volatility, momentum, zinc, collective, giving\n",
      "pace: modern, day, say, start, mine\n",
      "reason: called, longer, bought, government, times\n",
      "nuclear: spot, customer, owner, day, branch\n",
      "dnp: nio, let, consumer, room, outlets\n",
      "station: environmental, move, study, pace, start\n",
      "lines: flexi, modern, me, message, fairly\n",
      "agri: unwanted, smoothly, bicentenary, max, returns\n",
      "dynamic: suspension, drinks, day, tv, shareholder\n",
      "pzu: please, study, day, covid, environmental\n",
      "found: please, thus, removal, rapid, tv\n",
      "offices: my, little, court, almost, sole\n",
      "elcoteq: day, machinery, momentum, smart, study\n",
      "name: profile, consider, l, sought, claims\n",
      "tonnes: rejected, unlike, final, across, shareholder\n",
      "larger: leaders, metres, quick, till, role\n",
      "\n",
      "\n",
      "9901/9901 [==============================] - 293s 30ms/step - loss: 0.4961\n",
      "Epoch: 2/5 started\n",
      "   9867/Unknown - 298s 30ms/step - loss: 0.4271into: hel, slow, always, finnish, won\n",
      "system: loeb, creators, stuck, enters, pdz\n",
      "at: add, both, thus, attempt, slowed\n",
      "he: developer, break, led, because, submitted\n",
      "increase: application, unnamed, interview, customers, endorsement\n",
      "customers: competitive, machine, applications, pulp, processes\n",
      "data: valuation, went, eps, hel, intermodal\n",
      "pgnig: developed, meteorology, underway, emphasized, would\n",
      "have: has, recently, ca, supporting, treat\n",
      "related: ride, study, km, filtration, intensive\n",
      "compared: acting, mn, dominant, from, have\n",
      "among: futures, tourists, directions, over, increasing\n",
      "recommendation: cooperation, to, chassis, outlays, town\n",
      "bank: operational, belonging, krajowego, company, aldata\n",
      "industry: environment, watch, tulikivi, permanently, false\n",
      "this: specified, key, impulses, begin, monster\n",
      "voting: breath, possibly, conduct, standards, daily\n",
      "pace: st, surprisingly, dependent, jas, potentially\n",
      "reason: moving, highly, be, above, close\n",
      "nuclear: modular, identified, thursdays, guide, cim\n",
      "dnp: completed, store, submitted, highly, specialist\n",
      "station: years, practices, shelf, composite, certain\n",
      "lines: drink, mutual, rvi, ai, ranking\n",
      "agri: fuels, wanted, questions, smoothly, phase\n",
      "dynamic: cat, veneer, reliable, drinks, threat\n",
      "pzu: problems, camera, automatically, wins, bird\n",
      "found: pfe, marcel, king, premium, allowing\n",
      "offices: condition, content, negotiating, child, years\n",
      "elcoteq: dmasia, tvix, msft, eyes, follow\n",
      "name: wallet, cessation, controls, videos, kaupthing\n",
      "tonnes: grain, rejected, forex, ways, premarket\n",
      "larger: test, happy, sells, bottom, alleged\n",
      "\n",
      "\n",
      "9867/9867 [==============================] - 298s 30ms/step - loss: 0.4271\n",
      "Epoch: 3/5 started\n",
      "   9884/Unknown - 282s 29ms/step - loss: 0.3989into: consequence, finnish, invest, producer, hel\n",
      "system: her, disappointment, mass, manufacturers, expanded\n",
      "at: todays, thus, france, attempt, tetroxide\n",
      "he: that, what, just, reason, analyzes\n",
      "increase: extraordinary, signed, idiot, improvement, unnamed\n",
      "customers: twtr, goog, competitive, check, moving\n",
      "data: web, hel, version, valuation, single\n",
      "pgnig: cat, underway, developed, twtr, underground\n",
      "have: has, analyzing, designs, ca, recently\n",
      "related: of, filtration, upon, feed, presented\n",
      "compared: sq, usd, generates, newly, producing\n",
      "among: blocking, tourists, teleste, futures, indicators\n",
      "recommendation: remedies, bullish, constant, fiz, ranking\n",
      "bank: company, gon, event, rival, specialty\n",
      "industry: prices, secretary, technology, numerous, content\n",
      "this: bentos, crystals, capacities, slowed, reserved\n",
      "voting: attached, explains, enea, massively, rivals\n",
      "pace: st, strengthens, surprisingly, form, hit\n",
      "reason: rebound, upset, costly, thing, symbol\n",
      "nuclear: modular, rise, components, promise, terminal\n",
      "dnp: acquiring, mass, specialist, activated, completed\n",
      "station: practices, ciech, car, parquet, regulatory\n",
      "lines: oracle, intelligence, cmcsa, cholesterol, tycoon\n",
      "agri: sector, fuels, system, prices, ornamental\n",
      "dynamic: drinks, maritime, kilograms, tournament, super\n",
      "pzu: certification, opole, wooden, referred, sharp\n",
      "found: ropax, lotus, memory, maintained, watermarks\n",
      "offices: restriction, garden, mansi, premium, ie\n",
      "elcoteq: trades, event, oats, unfortunately, ual\n",
      "name: demanding, smoke, calciners, paving, weather\n",
      "tonnes: armada, rheumatoid, skillfully, boepd, outfits\n",
      "larger: sells, calculates, difficulties, cast, europol\n",
      "\n",
      "\n",
      "9884/9884 [==============================] - 282s 29ms/step - loss: 0.3989\n",
      "Epoch: 4/5 started\n",
      "   9865/Unknown - 310s 31ms/step - loss: 0.3775into: capturing, taxable, breath, rrgb, negatively\n",
      "system: profile, pond, invitation, export, installations\n",
      "at: thus, slowed, eet, citycon, france\n",
      "he: that, what, just, adopting, it\n",
      "increase: unnamed, improvement, interview, even, endorsement\n",
      "customers: rally, arm, go, wholly, clients\n",
      "data: lead, hel, ailus, factor, leaseback\n",
      "pgnig: eng, stojan, polyolefin, manufactures, eurusd\n",
      "have: has, killing, rangebound, recently, awarded\n",
      "related: of, filtration, ordered, individual, partial\n",
      "compared: usd, eur, dominant, in, sq\n",
      "among: wider, tourists, blocking, dc, directions\n",
      "recommendation: tire, competition, remedies, valuations, fiz\n",
      "bank: company, rutav, rtrks, connated, helsinki\n",
      "industry: agri, prices, machinery, provider, technology\n",
      "this: safe, which, key, communal, cc\n",
      "voting: attached, massively, mountain, ignatius, worsen\n",
      "pace: vendors, gentle, st, brk, appliances\n",
      "reason: close, extremely, definitely, break, indication\n",
      "nuclear: modular, grain, smr, videos, domain\n",
      "dnp: specialist, opened, activated, acquiring, mass\n",
      "station: practices, movements, sandwich, forced, formed\n",
      "lines: panel, boosted, wti, tallink, relations\n",
      "agri: sector, industry, prices, system, fuels\n",
      "dynamic: kilograms, tournament, cov, persistent, choice\n",
      "pzu: wallets, allianz, cqqq, reporters, boiled\n",
      "found: soapstone, mon, ccc, aid, ropax\n",
      "offices: barley, vishakapatnam, crypto, auto, parkano\n",
      "elcoteq: event, frn, tools, oats, scissors\n",
      "name: hame, rrc, arduously, profile, directionality\n",
      "tonnes: madison, creditors, demonstration, deteriorating, divergence\n",
      "larger: hit, hits, taken, sold, quick\n",
      "\n",
      "\n",
      "9865/9865 [==============================] - 310s 31ms/step - loss: 0.3775\n",
      "Epoch: 5/5 started\n",
      "   9863/Unknown - 297s 30ms/step - loss: 0.3591into: capturing, leasing, wanted, taxable, lye\n",
      "system: pond, installations, agri, mass, profile\n",
      "at: thus, theoretically, glassworldex, ahlstrom, citycon\n",
      "he: that, what, just, concern, break\n",
      "increase: decrease, declines, expert, invitation, interview\n",
      "customers: rally, lowered, go, check, gusta\n",
      "data: citing, hel, permit, adac, disappointing\n",
      "pgnig: richest, previously, wet, emphasized, proposals\n",
      "have: has, overbought, killing, awarded, rangebound\n",
      "related: escondida, ordered, filtration, implement, partial\n",
      "compared: usd, eur, sq, dominant, tons\n",
      "among: wider, tourists, fully, teleste, epidemiological\n",
      "recommendation: tire, concentration, porvoo, frm, party\n",
      "bank: company, rutav, connated, elcoteq, universal\n",
      "industry: agri, duty, provider, prices, technology\n",
      "this: revival, which, payment, develii, analysis\n",
      "voting: attached, leadership, option, ver, marcel\n",
      "pace: vendors, luxury, deaths, appliances, undisclosed\n",
      "reason: close, door, gain, financially, definitely\n",
      "nuclear: modular, grain, valga, videos, smr\n",
      "dnp: specialist, nio, timeframe, consistently, opened\n",
      "station: forced, practices, sandwich, mbank, iv\n",
      "lines: relations, panel, giving, heating, interior\n",
      "agri: industry, sector, prices, system, layer\n",
      "dynamic: merlin, popularity, choose, totally, component\n",
      "pzu: allianz, wallets, grudzie, expressed, left\n",
      "found: easing, riihim, ropax, kemij, km\n",
      "offices: automobile, ie, parkano, eurocash, crypto\n",
      "elcoteq: omeo, create, scissors, gray, fiskars\n",
      "name: entreprenor, directionality, rrc, specialist, xlf\n",
      "tonnes: divergence, madison, labu, kogeneration, overnight\n",
      "larger: bac, marcin, consultancy, cvx, oka\n",
      "\n",
      "\n",
      "9864/9864 [==============================] - 298s 30ms/step - loss: 0.3591\n"
     ]
    }
   ],
   "source": [
    "cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    news_cbow_gen = cbow_data_generator(news_sequences, window_size, batch_size, negative_samples)\n",
    "    cbow_model.fit(\n",
    "        news_cbow_gen, \n",
    "        epochs=1, \n",
    "        callbacks=cbow_validation_callback,         \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ee2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
    "        \n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"context_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"context_embedding_cbow.pkl\"))\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"target_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"target_embedding_cbiw.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d449b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(cbow_model, tokenizer, n_vocab, save_dir='cbow_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a46373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
