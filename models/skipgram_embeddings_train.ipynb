{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5009fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 17:27:44.222379: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-23 17:27:44.222416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58315a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from operator import itemgetter\n",
    "all_files=listdir('/home/slisowski/Portfolio/wig20_sentiment/translated_files_prod/embed/')\n",
    "\n",
    "#We have to sort files by index number, but this is string so first we need to extract index as integer\n",
    "file_idx=[]\n",
    "for file in all_files:\n",
    "    idx=file.split('_')[2]\n",
    "    file_idx.append(int(idx))\n",
    "file_idx\n",
    "\n",
    "files_idx=zip(all_files, file_idx)\n",
    "sorted_tuples=sorted(tuple(files_idx),key=itemgetter(1))\n",
    "sorted_list_files=[]\n",
    "for tup in sorted_tuples:\n",
    "    dir_path='/home/slisowski/Portfolio/wig20_sentiment/translated_files_prod/embed/'\n",
    "    file_path=dir_path+tup[0]\n",
    "    sorted_list_files.append(file_path)\n",
    "    \n",
    "\n",
    "    \n",
    "news_df=pd.concat(map(pd.read_csv, sorted_list_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bbaa576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>company</th>\n",
       "      <th>news_date</th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ASSECO-POLAND</td>\n",
       "      <td>2022-09-16 08:48:05</td>\n",
       "      <td>Thursday on the Polish stock exchange was anot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ASSECO-POLAND</td>\n",
       "      <td>2022-09-09 15:37:42</td>\n",
       "      <td>During the video conferencing, the president o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ASSECO-POLAND</td>\n",
       "      <td>2022-09-06 17:22:00</td>\n",
       "      <td>\"This year, and the whole future Asseco Poland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ASSECO-POLAND</td>\n",
       "      <td>2022-09-02 12:29:08</td>\n",
       "      <td>Given the fatal performance of both WIG20 and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ASSECO-POLAND</td>\n",
       "      <td>2022-09-01 10:35:52</td>\n",
       "      <td>Improving the results of the IT group was poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>295</td>\n",
       "      <td>WIG</td>\n",
       "      <td>2020-03-03 11:42:38</td>\n",
       "      <td>Increase in the value of trading in shares und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>296</td>\n",
       "      <td>WIG</td>\n",
       "      <td>2020-03-02 10:22:36</td>\n",
       "      <td>Delayed Coronavir's effect - this is how a com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>297</td>\n",
       "      <td>WIG</td>\n",
       "      <td>2020-02-28 12:18:54</td>\n",
       "      <td>During the turmoil associated with the Coronav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>298</td>\n",
       "      <td>WIG</td>\n",
       "      <td>2020-02-05 09:14:13</td>\n",
       "      <td>For years, we have been conducting original st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>299</td>\n",
       "      <td>WIG</td>\n",
       "      <td>2020-01-15 11:36:32</td>\n",
       "      <td>At the beginning of the 4th quarter of 2019, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12602 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        company            news_date  \\\n",
       "0               0  ASSECO-POLAND  2022-09-16 08:48:05   \n",
       "1               1  ASSECO-POLAND  2022-09-09 15:37:42   \n",
       "2               2  ASSECO-POLAND  2022-09-06 17:22:00   \n",
       "3               3  ASSECO-POLAND  2022-09-02 12:29:08   \n",
       "4               4  ASSECO-POLAND  2022-09-01 10:35:52   \n",
       "...           ...            ...                  ...   \n",
       "12597         295            WIG  2020-03-03 11:42:38   \n",
       "12598         296            WIG  2020-03-02 10:22:36   \n",
       "12599         297            WIG  2020-02-28 12:18:54   \n",
       "12600         298            WIG  2020-02-05 09:14:13   \n",
       "12601         299            WIG  2020-01-15 11:36:32   \n",
       "\n",
       "                                               news_text  \n",
       "0      Thursday on the Polish stock exchange was anot...  \n",
       "1      During the video conferencing, the president o...  \n",
       "2      \"This year, and the whole future Asseco Poland...  \n",
       "3      Given the fatal performance of both WIG20 and ...  \n",
       "4      Improving the results of the IT group was poss...  \n",
       "...                                                  ...  \n",
       "12597  Increase in the value of trading in shares und...  \n",
       "12598  Delayed Coronavir's effect - this is how a com...  \n",
       "12599  During the turmoil associated with the Coronav...  \n",
       "12600  For years, we have been conducting original st...  \n",
       "12601  At the beginning of the 4th quarter of 2019, t...  \n",
       "\n",
       "[12602 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3af025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df=pd.read_csv('/home/slisowski/Portfolio/tweets_news_sentiment/tweets_labelled.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a6b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_headlines_df=pd.read_csv('/home/slisowski/Portfolio/tweets_news_sentiment/all-data.csv', encoding=\"Windows-1252\", names=[\"sentiment\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915851f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams_corpus=pd.concat([news_df['news_text'], tweets_df['text'], news_headlines_df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25857681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import preprocessor as p\n",
    "from bs4 import BeautifulSoup\n",
    "import demoji\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clean_news_skipgrams(text):\n",
    "    \n",
    "    #remove emoji\n",
    "    text=demoji.replace(text,' ')\n",
    "    # use tweeter preprocessor to clean news\n",
    "    text=p.clean(text)\n",
    "    #remove html tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text=contractions.fix(text)\n",
    "    \n",
    "    \n",
    "    #remove accented characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "   \n",
    "   \n",
    "    #remove numbers\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    #tokenize text\n",
    "    text=text.lower()\n",
    "    \n",
    "    list_of_words=word_tokenize(text)\n",
    "    #words=[]\n",
    "    \n",
    "    text = ' '.join(list_of_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74e155b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams_words=[]\n",
    "for news in skipgrams_corpus.tolist():\n",
    "    \n",
    "    text=clean_news_skipgrams(news)\n",
    "    skipgrams_words.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80cb4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_sentences=[news for news in skipgrams_words if skipgrams_words.count(news)>1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbd644e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6927"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duplicate_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710fa9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22448"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgrams_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fbebda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams_words=list(set(skipgrams_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0397f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17749"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skipgrams_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "976ec0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stories=skipgrams_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eab2cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "     split=' '\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92e1a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20783\n",
      "\n",
      "Words at the top\n",
      "\t {'the': 1, 'of': 2, 'in': 3, 'to': 4, 'and': 5, 'a': 6, 'for': 7, 'on': 8, 'is': 9, 'will': 10}\n",
      "\n",
      "Words at the bottom\n",
      "\t {'airvana': 20773, 'femto': 20774, 'harmonization': 20775, 'dycom': 20776, 'dy': 20777, 'obstacle': 20778, 'economically': 20779, 'saarijarvi': 20780, 'fisker': 20781, 'commercialized': 20782}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f812697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)\n",
    "news_sequences=[x for x in news_sequences if len(x)>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb4bbd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample phrase: dnp dinopl opened new stores\n",
      "Sample word IDs: [1304, 1122, 805, 41, 320]\n",
      "\n",
      "Sample skip-grams\n",
      "\tInput: [1304, 1122] (['dnp', 'dinopl']) / Label: 1\n",
      "\tInput: [1122, 1304] (['dinopl', 'dnp']) / Label: 1\n",
      "\tInput: [1122, 805] (['dinopl', 'opened']) / Label: 1\n",
      "\tInput: [805, 1122] (['opened', 'dinopl']) / Label: 1\n",
      "\tInput: [805, 41] (['opened', 'new']) / Label: 1\n",
      "\tInput: [41, 805] (['new', 'opened']) / Label: 1\n",
      "\tInput: [41, 320] (['new', 'stores']) / Label: 1\n",
      "\tInput: [320, 41] (['stores', 'new']) / Label: 1\n",
      "\tInput: [1122, 5171] (['dinopl', 'aug']) / Label: 0\n",
      "\tInput: [805, 918] (['opened', 'assigned']) / Label: 0\n",
      "\tInput: [1304, 12373] (['dnp', 'amplifiers']) / Label: 0\n",
      "\tInput: [41, 11221] (['new', 'imv']) / Label: 0\n",
      "\tInput: [320, 6058] (['stores', 'cells']) / Label: 0\n",
      "\tInput: [1122, 2055] (['dinopl', 'papers']) / Label: 0\n",
      "\tInput: [805, 1757] (['opened', 'abroad']) / Label: 0\n",
      "\tInput: [41, 5519] (['new', 'seat']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "sample_word_ids = news_sequences[0][:5]\n",
    "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
    "print(f\"Sample phrase: {sample_phrase}\")\n",
    "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
    "\n",
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids, \n",
    "    vocabulary_size=n_vocab, \n",
    "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
    "    categorical=False, sampling_table=None, seed=None\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Sample skip-grams\")\n",
    "\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edd1057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample: [[1122]]\n",
      "Negative samples: [   0    4   59 6414  648   68  924  284 1547    5]\n",
      "true_expected_count: [[0.00098436]]\n",
      "sampled_expected_count: [5.4840094e-01 1.8420938e-01 1.8137142e-02 1.7244756e-04 1.7021832e-03\n",
      " 1.5805339e-02 1.1948386e-03 3.8685752e-03 7.1428146e-04 1.5792997e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 17:40:56.480279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-23 17:40:56.480310: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-23 17:40:56.480330: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (slawomir): /proc/driver/nvidia/version does not exist\n",
      "2022-09-23 17:40:56.481899: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sample_word_ids, \n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1, \n",
    "    window_size=window_size, negative_samples=0, shuffle=False,    \n",
    ")\n",
    "\n",
    "inputs, labels = np.array(inputs), np.array(labels)\n",
    "\n",
    "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
    "    # A true context word that appears in the context of the target\n",
    "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
    "    num_true=1, # number of true words per example\n",
    "    num_sampled=10,\n",
    "    unique=True,\n",
    "    range_max=n_vocab,            \n",
    "    name=\"negative_sampling\"\n",
    ")\n",
    "\n",
    "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
    "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
    "print(f\"true_expected_count: {true_expected_count}\")\n",
    "print(f\"sampled_expected_count: {sampled_expected_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df6bb196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
    "\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0770108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([10586,   348, 10586,  7952,   348,  6730,   348, 10586, 10587,\n",
      "       10587]), array([  11,    3,   22,   28,    4,    2,  129,    5, 6711,    5]))\n",
      "[1 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
    "    \n",
    "    rand_sequence_ids = np.arange(len(sequences))                    \n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "\n",
    "\n",
    "    for si in rand_sequence_ids:\n",
    "        \n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "            sequences[si], \n",
    "            vocabulary_size=vocab_size, \n",
    "            window_size=window_size, \n",
    "            negative_samples=0.0, \n",
    "            shuffle=False,\n",
    "            sampling_table=sampling_table,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        targets, contexts, labels = [], [], []\n",
    "        \n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            \n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "              true_classes=context_class,\n",
    "              num_true=1,\n",
    "              num_sampled=negative_samples,\n",
    "              unique=True,\n",
    "              range_max=vocab_size,              \n",
    "              name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            context = tf.concat(\n",
    "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
    "                axis=0\n",
    "            )\n",
    "            \n",
    "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            \n",
    "            targets.extend([target_word]*(negative_samples+1))\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "        if (len(contexts)==0) | (len(targets)==0) | (len(labels)==0):\n",
    "            continue\n",
    "\n",
    "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
    "        \n",
    "        assert contexts.shape[0] == targets.shape[0]\n",
    "        assert contexts.shape[0] == labels.shape[0]\n",
    "        \n",
    "        # If seed is not provided generate a random one\n",
    "        if not seed:\n",
    "            seed = random.randint(0, 10e6)\n",
    "            \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(contexts)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(targets)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(labels)\n",
    "        \n",
    "    \n",
    "        for eg_id_start in range(0, contexts.shape[0], batch_size):            \n",
    "            yield (\n",
    "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])], \n",
    "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
    "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
    "\n",
    "\n",
    "news_skip_gram_gen = skip_gram_data_generator(\n",
    "    news_sequences, 4, 10, 5, n_vocab\n",
    ")\n",
    "\n",
    "for btc, bl in news_skip_gram_gen:\n",
    "    \n",
    "    print(btc)\n",
    "    print(bl)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7afc3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # Data points in a single batch\n",
    "\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "\n",
    "window_size=1 # We use a window size of n on either side of target word\n",
    "negative_samples = 4 # Number of negative samples generated per example\n",
    "\n",
    "epochs = 10 # Number of epochs to train for\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(\n",
    "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "    axis=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e702ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 128)          2660224     ['context[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          2660224     ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,320,448\n",
      "Trainable params: 5,320,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Inputs - skipgrams() function outputs target, context in that order\n",
    "# we will use the same order\n",
    "input_1 = tf.keras.layers.Input(shape=(), name='target')\n",
    "input_2 = tf.keras.layers.Input(shape=(), name='context')\n",
    "\n",
    "# Two embeddings layers are used one for the context and one for the target\n",
    "context_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
    ")\n",
    "target_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
    ")\n",
    "\n",
    "# Lookup outputs of the embedding layers\n",
    "target_out = target_embedding_layer(input_1)\n",
    "context_out = context_embedding_layer(input_2)\n",
    "\n",
    "# Computing the dot product between the two \n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "# Defining the model\n",
    "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='skip_gram_model')\n",
    "\n",
    "# Compiling the model\n",
    "skip_gram_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "skip_gram_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2af7293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "        \n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\" Validation logic \"\"\"\n",
    "                \n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "        \n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "        \n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "                \n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "            \n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1c82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 started\n",
      "  17013/Unknown - 850s 50ms/step - loss: 0.4569day: logs, lubiatw, marketing, dip, boasted\n",
      "term: sized, internal, stays, kaido, kryska\n",
      "s: shoes, bags, j, whole, biocng\n",
      "stock: permanent, release, gpw, spread, giving\n",
      "first: second, third, crucial, engineer, rzeszw\n",
      "compared: chf, emphasize, fill, jnk, inventors\n",
      "most: extremely, explosion, suffered, gry, found\n",
      "investment: whammy, mrl, grad, powerhouse, strives\n",
      "pkn: lietuva, promotional, wants, poudnie, solidarity\n",
      "march: watchnow, actual, contrasting, searches, jamal\n",
      "pekao: santander, peo, deutsche, meets, mied\n",
      "revenues: dropped, decreased, settled, strongest, muller\n",
      "sector: dress, gry, banki, imho, daysfibonacci\n",
      "also: fangdd, entrusted, little, impossible, warranties\n",
      "data: pushes, initiates, analyzes, crosshairs, moutai\n",
      "profit: result, loss, corrected, consolidated, profits\n",
      "ready: ranges, cup, advance, edt, imposing\n",
      "emission: arabia, sintered, enb, sounds, broadband\n",
      "yet: rules, decide, vaisala, pointing, rmb\n",
      "asset: document, recap, hrywn, meant, tun\n",
      "l: tehtaat, staycation, plate, inverted, wti\n",
      "view: ln, eiof, dealing, stonk, friend\n",
      "treasury: lasting, condition, hipoteczny, talk\n",
      "norway: junk, clong, interaction, killer, appl\n",
      "bynd: pton, roku, nflx, snap, amzn\n",
      "plants: blocks, distinguished, opole, shown, variable\n",
      "continues: cooperate, appetite, foresees, shit, penalty\n",
      "settlement: availability, admission, suspending, possibility\n",
      "show: request, possibility, difference, vote\n",
      "advantage: beautifully, limit, jyvaeskylae, furnaces, waiting\n",
      "former: slice, announce, nederland, vice, overpayment\n",
      "expert: occupied, talk, pessimistic, difference\n",
      "\n",
      "\n",
      "17014/17014 [==============================] - 850s 50ms/step - loss: 0.4569\n",
      "Epoch: 2/10 started\n",
      "  17034/Unknown - 836s 49ms/step - loss: 0.3992day: opportunity, derstr, steal, return, lubiatw\n",
      "term: sized, phones, rationalization, barbr, receptacles\n",
      "s: plc, oyj, skog, r, shoes\n",
      "stock: release, gpw, permanent, issues, spread\n",
      "first: second, third, emergency, salaried, communal\n",
      "compared: subcontract, detained, chf, defeated, fill\n",
      "most: exerted, lesson, except, epg, scalp\n",
      "investment: grad, umbrella, strives, stake, detailed\n",
      "pkn: promotional, moon, lietuva, reproduction, dipping\n",
      "march: february, bilateral, december, narrative, opoka\n",
      "pekao: polski, santander, skiba, peo, pzu\n",
      "revenues: influenced, capitalization, dropped, turnover, prior\n",
      "sector: clothing, agri, renewable, views, cleans\n",
      "also: commercially, tblt, completely, still, managers\n",
      "data: seeing, analyzes, peaks, trojan, altus\n",
      "profit: result, loss, repetitive, corrected, ebit\n",
      "ready: syssy, imposing, memorandum, slopes, occupied\n",
      "emission: will, directional, maturity, significant, closing\n",
      "yet: euronft, exclude, rules, autogas, impossible\n",
      "asset: financeconfluence, fuel, item, newsweek, arrangement\n",
      "l: tehtaat, crwd, spxl, tnt, unitra\n",
      "view: often, ayro, adia, awaited, sizes\n",
      "treasury: lists, telling, ministers, headbox, intermediary\n",
      "norway: canada, assertive, commissioning, amidst, ruble\n",
      "bynd: roku, pton, gs, nflx, tsla\n",
      "plants: blocks, opole, honestly, variable, admission\n",
      "continues: appetite, motofocus, ctyx, kardzhali, describe\n",
      "settlement: jamal, dominance, accordance, availability\n",
      "show: turns, inverted, strategist, ree, possibility\n",
      "advantage: retreat, chopped, olds, behavior, surprises\n",
      "former: eyen, recognizable, systemic, supervisory, deutsche\n",
      "expert: intention, buffer, vote, imposing, dominated\n",
      "\n",
      "\n",
      "17034/17034 [==============================] - 836s 49ms/step - loss: 0.3992\n",
      "Epoch: 3/10 started\n",
      "  17066/Unknown - 838s 49ms/step - loss: 0.3556day: return, oxygen, ominous, tyg, atmosphere\n",
      "term: sized, phones, negotiates, amounting, kryska\n",
      "s: headquartered, plc, nnen, aims, oriola\n",
      "stock: gpw, release, permanent, spread, neutral\n",
      "first: second, third, emergency, salaried, summer\n",
      "compared: copenhagen, ltm, chf, pride, delist\n",
      "most: lesson, rtv, sponsors, burden, thing\n",
      "investment: ip, nastola, grad, financing, carriers\n",
      "pkn: wocawek, poudnie, implementing, headed, lietuva\n",
      "march: february, december, zdzikot, relates, walking\n",
      "pekao: polski, skiba, peo, pogorzelski, czarnecki\n",
      "revenues: consolidated, turnover, net, sales, result\n",
      "sector: clothing, agri, renewable, responsibility, led\n",
      "also: spxanyone, positively, registry, commercially, completely\n",
      "data: cpi, analyzes, pushes, calcination, formula\n",
      "profit: result, loss, taxes, repetitive, corrected\n",
      "ready: memorandum, entirely, vaults, imposing, bkx\n",
      "emission: personnel, payments, dynamic, broadband, haste\n",
      "yet: directly, impossible, bavelloni, easily, hepatocellular\n",
      "asset: instruments, financeconfluence, casualty, winiarski, linden\n",
      "l: tehtaat, nen, attracts, wcdma, tnt\n",
      "view: adia, grass, bessa, goat, assignment\n",
      "treasury: kdziora, length, telling, survival, stabilized\n",
      "norway: amidst, strengths, muller, cows, lkab\n",
      "bynd: roku, pton, gs, dkng, tsla\n",
      "plants: blocks, linkspans, agents, opole, fit\n",
      "continues: sudbury, echo, milestones, abev, apparently\n",
      "settlement: jamal, odd, accordance, consequence\n",
      "show: supposed, kept, preview, buczek, karpacz\n",
      "advantage: behavior, minsrises, cnna, surprises, gw\n",
      "former: stoen, maggie, pda, blast, vast\n",
      "expert: underneath, carry, intention, slips, opera\n",
      "\n",
      "\n",
      "17066/17066 [==============================] - 839s 49ms/step - loss: 0.3556\n",
      "Epoch: 4/10 started\n",
      "  17044/Unknown - 801s 47ms/step - loss: 0.3223day: happen, morningand, oxygen, seasonality, night\n",
      "term: sized, phones, kaido, kernel, horizon\n",
      "s: headquartered, armed, nnen, oriola, agricultural\n",
      "stock: gpw, spread, divergencedate, neutral, release\n",
      "first: second, third, fireworks, summer, ipko\n",
      "compared: ltm, tag, eur, hollola, exceed\n",
      "most: lesson, thing, rtv, surges, equally\n",
      "investment: dkr, ip, andrusiewicz, dai, pfr\n",
      "pkn: poudnie, agd, pacza, spw, setbacks\n",
      "march: february, december, rinkuskiai, cornerstone, impacts\n",
      "pekao: polski, skiba, peo, pogorzelski, czarnecki\n",
      "revenues: turnover, capitalization, consolidated, influenced, slowly\n",
      "sector: agri, dress, responsibility, cemetery, boughtreal\n",
      "also: spxanyone, bewi, boring, registry, completely\n",
      "data: analysi, formula, cpi, calcination, analyzes\n",
      "profit: result, ebit, loss, corrected, repetitive\n",
      "ready: wanted, pctl, modernize, memorandum, epicenter\n",
      "emission: blurred, guder, dynamic, debutant, payments\n",
      "yet: bavelloni, impossible, telcontar, easily, directly\n",
      "asset: kujawa, corporations, casualty, instruments, goldman\n",
      "l: tehtaat, nen, wcdma, kmi, attracts\n",
      "view: frkn, bessa, responsive, indi, assignment\n",
      "treasury: kdziora, solstice, survival, length, relaunch\n",
      "norway: lkab, marketaxess, cows, ireland, netherlands\n",
      "bynd: roku, hpq, pton, bidu, aal\n",
      "plants: generator, eliminating, implied, agricultural, fit\n",
      "continues: adani, abev, protest, age, hoursnyse\n",
      "settlement: conversion, reiterate, engineer, jamal, accordance\n",
      "show: supposed, buczek, putcallratio, karpacz, retreat\n",
      "advantage: behavior, minsrises, atmosphere, relaxation, kach\n",
      "former: supervisory, pda, liitto, visited, lca\n",
      "expert: opera, underneath, hires, digits, integrate\n",
      "\n",
      "\n",
      "17044/17044 [==============================] - 801s 47ms/step - loss: 0.3223\n",
      "Epoch: 5/10 started\n",
      "  17024/Unknown - 797s 47ms/step - loss: 0.2970day: happen, atmosphere, opportunity, seed, morningand\n",
      "term: sized, horizon, kernel, bronsko, collapse\n",
      "s: headquartered, shoes, nnen, atomic, oriola\n",
      "stock: floated, gpw, seaspine, amrx, eols\n",
      "first: second, third, fireworks, summer, ipko\n",
      "compared: ltm, silent, eur, subcontract, copenhagen\n",
      "most: rtv, lesson, burden, except, thing\n",
      "investment: soh, dkr, delisted, expense, pfr\n",
      "pkn: poudnie, wocawska, spw, nest, istanbul\n",
      "march: february, december, bilateral, impacts, rinkuskiai\n",
      "pekao: skiba, peo, polski, czarnecki, borys\n",
      "revenues: turnover, influenced, capitalization, bricks, baltics\n",
      "sector: agri, sdcm, bbilatest, tubes, cemetery\n",
      "also: spxanyone, variables, soars, boring, bbox\n",
      "data: chesapeake, peaks, solteq, pitfalls, analysi\n",
      "profit: result, ebit, loss, repetitive, corrected\n",
      "ready: contain, elec, altia, wanted, chore\n",
      "emission: blurred, refueling, guder, titular, ruukki\n",
      "yet: directly, markethighly, disturbing, lagging, laying\n",
      "asset: kujawa, goldman, instruments, casualty, nakd\n",
      "l: tehtaat, nen, attracts, kmi, sydney\n",
      "view: orchard, bessa, frkn, ssici, responsive\n",
      "treasury: expresses, loudeac, ej, kdziora, length\n",
      "norway: lkab, ireland, marketaxess, netherlands, junction\n",
      "bynd: roku, hpq, aal, dkng, slv\n",
      "plants: generator, fit, opole, agents, prohouse\n",
      "continues: baird, veru, monitoring, podiumthe, adani\n",
      "settlement: jamal, safest, deliberately, engineer, addressed\n",
      "show: supposed, karpacz, kept, brinker, putcallratio\n",
      "advantage: behavior, minsrises, stratification, satisfied, relaxation\n",
      "former: pda, pth, logo, blast, pte\n",
      "expert: opera, boardman, adjusted, drink, observing\n",
      "\n",
      "\n",
      "17024/17024 [==============================] - 797s 47ms/step - loss: 0.2970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 started\n",
      "  17021/Unknown - 818s 48ms/step - loss: 0.2768day: seed, night, happen, atmosphere, roc\n",
      "term: sized, collapse, bronsko, swot, horizon\n",
      "s: headquartered, atomic, nnen, oriola, recari\n",
      "stock: floated, seaspine, cannabis, swing, approached\n",
      "first: second, third, ipko, fourth, shelves\n",
      "compared: ltm, silent, cx, slipping, totalenergies\n",
      "most: rtv, lesson, burden, casual, testament\n",
      "investment: dkr, soh, wisely, successor, sts\n",
      "pkn: poudnie, wocawska, pacza, siemidarno, shortened\n",
      "march: february, december, prejudge, rinkuskiai, paneuro\n",
      "pekao: skiba, polski, czarnecki, peo, pogorzelski\n",
      "revenues: turnover, baltics, capitalization, influenced, bricks\n",
      "sector: agri, sdcm, racked, woman, spyback\n",
      "also: spxanyone, selections, variables, boring, underway\n",
      "data: pitfalls, shield, disappointing, commentary, debuted\n",
      "profit: ebit, result, loss, taxes, repetitive\n",
      "ready: ct, recruit, modernize, gup, omit\n",
      "emission: stance, refueling, cleaned, invalid, expense\n",
      "yet: disturbing, lagging, bears, markethighly, laying\n",
      "asset: kujawa, newsweek, goldman, skittles, instruments\n",
      "l: tehtaat, nen, backlogs, tikkakoski, sydney\n",
      "view: ssici, responsive, orchard, smug, envisaged\n",
      "treasury: expresses, relaunch, solstice, loudeac, length\n",
      "norway: ireland, lulea, netherlands, marketaxess, dialogue\n",
      "bynd: hpq, roku, aal, bidu, nvda\n",
      "plants: plant, agents, fit, opole, glassfiber\n",
      "continues: forming, faanmgs, monitoring, churning, baird\n",
      "settlement: jamal, safest, deliberately, eyesight, archicad\n",
      "show: brinker, solidium, supposed, uruguay, inability\n",
      "advantage: behavior, relaxation, indistinguishable, minsrises, satisfied\n",
      "former: logo, presidential, pth, pte, maythe\n",
      "expert: opera, severity, recognizes, playway, observing\n",
      "\n",
      "\n",
      "17021/17021 [==============================] - 818s 48ms/step - loss: 0.2768\n",
      "Epoch: 7/10 started\n",
      "  17084/Unknown - 817s 48ms/step - loss: 0.2601day: seed, morningand, happen, night, opportunity\n",
      "term: sized, horizon, kernel, historical, swot\n",
      "s: headquartered, metsa, nnen, oriola, armed\n",
      "stock: seaspine, floated, gpw, vaisala, cannabis\n",
      "first: second, third, fourth, ipko, smile\n",
      "compared: cx, silent, ltm, words, totalenergies\n",
      "most: rtv, lesson, burden, equally, trademarks\n",
      "investment: dkr, soh, ofes, delisted, capex\n",
      "pkn: poudnie, pacza, shortened, petrochemistry, wocawska\n",
      "march: february, december, rinkuskiai, paneuro, alexandria\n",
      "pekao: skiba, polski, peo, czarnecki, hipoteczny\n",
      "revenues: turnover, baltics, gk, eek, influenced\n",
      "sector: agri, soxl, spyback, cemetery, bondspot\n",
      "also: selections, spxanyone, retracement, rarely, bewi\n",
      "data: shield, disappointing, commentary, screeners, cpi\n",
      "profit: ebit, result, loss, finnlines, corrected\n",
      "ready: explaining, recruit, bkx, ct, modernize\n",
      "emission: titular, refueling, ppe, stance, oyname\n",
      "yet: disturbing, everything, markethighly, bears, hardest\n",
      "asset: kujawa, goldman, perfumes, componenta, reiterates\n",
      "l: tehtaat, nen, mastercard, backlogs, flatiron\n",
      "view: ssici, orchard, responsive, wellness, envisaged\n",
      "treasury: expresses, relaunch, solstice, demolition, publish\n",
      "norway: ireland, lulea, marketaxess, netherlands, orlen\n",
      "bynd: roku, hpq, dal, bidu, nvda\n",
      "plants: fit, linkspans, glassfiber, generator, agents\n",
      "continues: plx, tragedy, baird, veru, exhibit\n",
      "settlement: jamal, safest, fisas, bosse, deputies\n",
      "show: brinker, karpacz, consequences, recovering, dragged\n",
      "advantage: behavior, cnna, gratitude, relaxation, sling\n",
      "former: logo, presidential, pth, worstby, pda\n",
      "expert: opera, severity, observing, edt, lack\n",
      "\n",
      "\n",
      "17085/17085 [==============================] - 817s 48ms/step - loss: 0.2601\n",
      "Epoch: 8/10 started\n",
      "  17015/Unknown - 796s 47ms/step - loss: 0.2452day: morningand, intense, night, opportunity, seed\n",
      "term: sized, huuge, historical, horizon, swot\n",
      "s: armed, headquartered, nnen, vodafone, brewery\n",
      "stock: seaspine, vaisala, cannabis, floated, amrx\n",
      "first: second, third, ipko, smile, fourth\n",
      "compared: ltm, bbl, relation, silent, eek\n",
      "most: rtv, lesson, burden, occasions, compact\n",
      "investment: dkr, soh, undertaking, delisted, contempus\n",
      "pkn: poudnie, pacza, argenta, lietuva, ifm\n",
      "march: february, december, alexandria, rinkuskiai, ervi\n",
      "pekao: skiba, polski, borys, czarnecki, pogorzelski\n",
      "revenues: eek, baltics, beras, turnover, nfoigw\n",
      "sector: agri, soxl, sdcm, cemetery, seventy\n",
      "also: selections, rarely, modification, retracement, bewi\n",
      "data: shield, disappointing, debuted, commentary, results\n",
      "profit: result, ebit, loss, repetitive, corrected\n",
      "ready: uninterrupted, mommy, tall, bkx, advac\n",
      "emission: ppe, expense, pbll, arranger, granulation\n",
      "yet: disturbing, loaders, joyfully, lagging, disagree\n",
      "asset: kujawa, perfumes, reiterates, pack, goldman\n",
      "l: tehtaat, nen, backlogs, attracts, mastercard\n",
      "view: ssici, indi, responsive, intense, envisaged\n",
      "treasury: solstice, ukowice, relaunch, length, divest\n",
      "norway: ireland, lulea, portugal, willingly, poultry\n",
      "bynd: hpq, roku, pton, dal, bidu\n",
      "plants: develop, glassfiber, fit, linkspans, cdpprojekt\n",
      "continues: plx, forming, baird, talvivaara, aume\n",
      "settlement: jamal, safest, comfort, wz, fisas\n",
      "show: dragged, uruguay, examines, recovering, prescriptions\n",
      "advantage: usdspx, careers, gratitude, availability, behavior\n",
      "former: logo, presidential, pth, understandable, isbnews\n",
      "expert: severity, lack, opera, resembles, observing\n",
      "\n",
      "\n",
      "17015/17015 [==============================] - 796s 47ms/step - loss: 0.2452\n",
      "Epoch: 9/10 started\n",
      "  17047/Unknown - 798s 47ms/step - loss: 0.2363day: morningand, night, intense, seed, phonebook\n",
      "term: sized, huuge, horizon, confusion, established\n",
      "s: armed, headquartered, swung, metsa, maggie\n",
      "stock: floated, seaspine, polygon, vaisala, msci\n",
      "first: second, ipko, fourth, third, zahariev\n",
      "compared: ltm, chf, relation, liters, uncommon\n",
      "most: compact, lesson, rtv, equally, condolences\n",
      "investment: soh, dkr, biocidal, pfr, nog\n",
      "pkn: poudnie, pacza, ifm, argenta, wrotkw\n",
      "march: february, december, ervi, rinkuskiai, expandable\n",
      "pekao: skiba, borys, peo, polski, registration\n",
      "revenues: eek, gk, beras, baltics, influenced\n",
      "sector: soxl, agri, sdcm, cows, cemetery\n",
      "also: selections, rarely, modification, dpd, bewi\n",
      "data: debuted, disappointing, shield, analysi, solteq\n",
      "profit: result, ebit, repetitive, corrected, taxes\n",
      "ready: uninterrupted, bkx, resale, pctl, advac\n",
      "emission: ppe, granulation, binding, pbll, finding\n",
      "yet: disturbing, pfd, joyfully, loaders, lagging\n",
      "asset: kujawa, cues, perfumes, reiterates, forests\n",
      "l: tehtaat, backlogs, nen, attracts, nail\n",
      "view: ssici, bbio, negotiators, worlds, wellness\n",
      "treasury: solstice, length, publish, ukowice, relaunch\n",
      "norway: ireland, lulea, orlen, germans, copenhagen\n",
      "bynd: hpq, roku, pton, nnvc, dal\n",
      "plants: develop, rename, worldthe, glassfiber, durable\n",
      "continues: plx, aume, forming, pilgrimsdaily, rohwedder\n",
      "settlement: wz, digitally, safest, jamal, pipette\n",
      "show: dragged, recovering, groclin, irrelevant, prescriptions\n",
      "advantage: sociedad, usdspx, size, dictation, organize\n",
      "former: presidential, logo, pte, sponsoring, worstby\n",
      "expert: opera, severity, lack, grmm, ridden\n",
      "\n",
      "\n",
      "17047/17047 [==============================] - 798s 47ms/step - loss: 0.2363\n",
      "Epoch: 10/10 started\n",
      "  17048/Unknown - 795s 47ms/step - loss: 0.2269day: morningand, phonebook, hammeris, positioned, seed\n",
      "term: sized, huuge, confusion, horizon, eloholma\n",
      "s: swung, nnen, maggie, headquartered, anderson\n",
      "stock: polygon, floated, seaspine, vaisala, leans\n",
      "first: second, ipko, pushers, fifteed, third\n",
      "compared: ltm, eek, yoy, riley, immune\n",
      "most: compact, lesson, rtv, trademarks, vacon\n",
      "investment: dkr, soh, biocidal, undertaking, shareholding\n",
      "pkn: poudnie, pacza, ifm, shortened, wocawska\n",
      "march: february, december, activated, ervi, alexandria\n",
      "pekao: skiba, polski, registration, borys, czarnecki\n",
      "revenues: eek, beras, baltics, gk, turnover\n",
      "sector: soxl, sdcm, agri, dress, cemetery\n",
      "also: selections, retracement, lmnd, dpd, rarely\n",
      "data: debuted, disappointing, commentary, guarantees, promise\n",
      "profit: result, corrected, ebit, repetitive, taxes\n",
      "ready: mommy, uninterrupted, advac, entirely, pctl\n",
      "emission: ppe, positivelyukrainian, listing, granulation, individualized\n",
      "yet: disturbing, joyfully, phenomenon, loaders, pfd\n",
      "asset: kujawa, cues, massively, uruguayan, ebury\n",
      "l: tehtaat, backlogs, nen, littered, nail\n",
      "view: ssici, iaas, renew, responsive, lockers\n",
      "treasury: solstice, length, demolition, ukowice, waymo\n",
      "norway: ireland, lulea, orlen, netherlands, poultry\n",
      "bynd: roku, hpq, pton, bidu, dal\n",
      "plants: worldthe, glassfiber, han, bechatw, obligation\n",
      "continues: plx, talvivaara, aume, forming, tyumen\n",
      "settlement: paviljonki, billions, digitally, wz, topping\n",
      "show: dragged, recovering, prescriptions, appreciate, irrelevant\n",
      "advantage: sociedad, usdspx, size, empik, preceding\n",
      "former: angela, presidential, pte, sponsoring, pth\n",
      "expert: opera, severity, lack, grmm, ridden\n",
      "\n",
      "\n",
      "17048/17048 [==============================] - 795s 47ms/step - loss: 0.2269\n"
     ]
    }
   ],
   "source": [
    "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    \n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    \n",
    "    news_skip_gram_gen = skip_gram_data_generator(\n",
    "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
    "    )\n",
    "    \n",
    "    skip_gram_model.fit(\n",
    "        news_skip_gram_gen, epochs=1, \n",
    "        callbacks=skipgram_validation_callback,        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce854d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
    "        \n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"context_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"context_embedding_skipgrams.pkl\"))\n",
    "    \n",
    "    pd.DataFrame(\n",
    "        model.get_layer(\"target_embedding\").get_weights()[0], \n",
    "        index = words_sorted\n",
    "    ).to_pickle(os.path.join(save_dir, \"target_embedding_skipgrams.pkl\"))\n",
    "\n",
    "    \n",
    "save_embeddings(skip_gram_model, tokenizer, n_vocab, save_dir='skipgram_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27033a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
